{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I don't Know why I keep it, but it works ╮(￣▽￣\")╭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pip_install(*packages):\n",
    "    '''\n",
    "    Author: Sy Hwang, Hollis Nolan, Shuheng Liu \n",
    "    \n",
    "    Install packages using pip\n",
    "    Alternatively just use command line\n",
    "    pip install package_name\n",
    "    '''\n",
    "    import pip\n",
    "    if int(pip.__version__.split(\".\")[0]) >= 10:  # since 10.0.0, pip does not support pip.main() any more\n",
    "        from pip._internal import main\n",
    "    else:\n",
    "        from pip import main\n",
    "\n",
    "    for package in packages:\n",
    "        try:\n",
    "            main([\"install\", \"--upgrade\", package, \"--user\"])\n",
    "        except Exception as e:\n",
    "            print(\"Unable to install {} using pip.\".format(package))\n",
    "            print(\"Exception:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: gensim in c:\\users\\pipi\\appdata\\roaming\\python\\python37\\site-packages (3.8.0)\n",
      "Requirement already satisfied, skipping upgrade: smart-open>=1.7.0 in c:\\users\\pipi\\appdata\\roaming\\python\\python37\\site-packages (from gensim) (1.8.4)\n",
      "Requirement already satisfied, skipping upgrade: six>=1.5.0 in c:\\users\\pipi\\anaconda3\\lib\\site-packages (from gensim) (1.12.0)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.11.3 in c:\\users\\pipi\\anaconda3\\lib\\site-packages (from gensim) (1.16.2)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.18.1 in c:\\users\\pipi\\anaconda3\\lib\\site-packages (from gensim) (1.2.1)\n",
      "Requirement already satisfied, skipping upgrade: boto>=2.32 in c:\\users\\pipi\\anaconda3\\lib\\site-packages (from smart-open>=1.7.0->gensim) (2.49.0)\n",
      "Requirement already satisfied, skipping upgrade: requests in c:\\users\\pipi\\anaconda3\\lib\\site-packages (from smart-open>=1.7.0->gensim) (2.21.0)\n",
      "Requirement already satisfied, skipping upgrade: boto3 in c:\\users\\pipi\\appdata\\roaming\\python\\python37\\site-packages (from smart-open>=1.7.0->gensim) (1.9.198)\n",
      "Requirement already satisfied, skipping upgrade: urllib3<1.25,>=1.21.1 in c:\\users\\pipi\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.7.0->gensim) (1.24.1)\n",
      "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in c:\\users\\pipi\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.7.0->gensim) (3.0.4)\n",
      "Requirement already satisfied, skipping upgrade: idna<2.9,>=2.5 in c:\\users\\pipi\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.7.0->gensim) (2.8)\n",
      "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in c:\\users\\pipi\\anaconda3\\lib\\site-packages (from requests->smart-open>=1.7.0->gensim) (2019.3.9)\n",
      "Requirement already satisfied, skipping upgrade: s3transfer<0.3.0,>=0.2.0 in c:\\users\\pipi\\appdata\\roaming\\python\\python37\\site-packages (from boto3->smart-open>=1.7.0->gensim) (0.2.1)\n",
      "Requirement already satisfied, skipping upgrade: botocore<1.13.0,>=1.12.198 in c:\\users\\pipi\\appdata\\roaming\\python\\python37\\site-packages (from boto3->smart-open>=1.7.0->gensim) (1.12.198)\n",
      "Requirement already satisfied, skipping upgrade: jmespath<1.0.0,>=0.7.1 in c:\\users\\pipi\\appdata\\roaming\\python\\python37\\site-packages (from boto3->smart-open>=1.7.0->gensim) (0.9.4)\n",
      "Requirement already satisfied, skipping upgrade: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in c:\\users\\pipi\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.198->boto3->smart-open>=1.7.0->gensim) (2.8.0)\n",
      "Requirement already satisfied, skipping upgrade: docutils<0.15,>=0.10 in c:\\users\\pipi\\anaconda3\\lib\\site-packages (from botocore<1.13.0,>=1.12.198->boto3->smart-open>=1.7.0->gensim) (0.14)\n",
      "Collecting nltk\n",
      "  Using cached https://files.pythonhosted.org/packages/87/16/4d247e27c55a7b6412e7c4c86f2500ae61afcbf5932b9e3491f8462f8d9e/nltk-3.4.4.zip\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not install packages due to an EnvironmentError: [Errno 2] No such file or directory: 'C:\\\\Users\\\\pipi\\\\AppData\\\\Local\\\\Temp\\\\pip-req-tracker-2tfhw0mn\\\\b3095b1324fb85a822887988cbb36067fa60285c59804dcfe133f1c4'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip_install('gensim', 'nltk')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find whether a Str is just a number\n",
    "def is_number(s):\n",
    "    try: \n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:  \n",
    "        pass \n",
    "    try:\n",
    "        import unicodedata  \n",
    "        unicodedata.numeric(s)  \n",
    "        return True\n",
    "    except (TypeError, ValueError):\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "\n",
    "#Find whether a Str contain English words or just meanless symbols\n",
    "def containenglish(str0):\n",
    "    import re\n",
    "    return bool(re.search('[a-z]', str0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean all non-English reviews and meanless reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from langdetect import detect\n",
    "import types\n",
    "\n",
    "#Load movie reviews dataset\n",
    "df = pd.read_csv( 'movie_reviews.csv', nrows=100000)\n",
    "texts = df.text.values #pd.Series -> np.ndarray\n",
    "label = df.label.values\n",
    "\n",
    "#Create null lists \n",
    "Eng_list = []\n",
    "label_list = []\n",
    "\n",
    "#clean data\n",
    "for i in range(len(label)): \n",
    "    if containenglish(texts[i]) == True:\n",
    "        if (detect(texts[i]) == 'en'):\n",
    "            Eng_list.append(texts[i])\n",
    "            label_list.append(label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# Transform each review string as a list of token strings. May take a few seconds\n",
    "tokenized = [nltk.word_tokenize(review) for review in Eng_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from itertools import chain\n",
    "\n",
    "def clean_text(tokenized_list, sw, punct, lemmatize=False):\n",
    "    new_list = []\n",
    "    for doc in tokenized_list:\n",
    "        new_list.append([token.lower() for token in doc if token.lower() not in chain(punct, sw)])\n",
    "    return new_list\n",
    "\n",
    "# Remove punctuations and stopwords, and lower-case text\n",
    "sw = stopwords.words('english')\n",
    "punct = punctuation\n",
    "cleaned = clean_text(tokenized, sw, punct)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Doc2Vec Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'pattern' package not found; tag filters are not available for English\n",
      "collecting all words and their counts\n",
      "PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "PROGRESS: at example #10000, processed 1432016 words (7635877/s), 60879 word types, 10000 tags\n",
      "collected 63346 word types and 10788 unique tags from a corpus of 10788 examples and 1548597 words\n",
      "Loading a fresh vocabulary\n",
      "effective_min_count=5 retains 15255 unique words (24% of original 63346, drops 48091)\n",
      "effective_min_count=5 leaves 1472902 word corpus (95% of original 1548597, drops 75695)\n",
      "deleting the raw counts dictionary of 63346 items\n",
      "sample=0.001 downsamples 45 most-common words\n",
      "downsampling leaves estimated 1118205 word corpus (75.9% of prior 1472902)\n",
      "estimated required memory for 15255 words and 300 dimensions: 57185100 bytes\n",
      "resetting layer weights\n",
      "training model with 3 workers on 15255 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "EPOCH 1 - PROGRESS: at 98.20% examples, 1106399 words/s, in_qsize 4, out_qsize 0\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 1 : training on 1548597 raw words (1129535 effective words) took 1.0s, 1106413 effective words/s\n",
      "EPOCH 2 - PROGRESS: at 98.14% examples, 1106253 words/s, in_qsize 4, out_qsize 0\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 2 : training on 1548597 raw words (1129123 effective words) took 1.0s, 1108588 effective words/s\n",
      "EPOCH 3 - PROGRESS: at 96.99% examples, 1090986 words/s, in_qsize 5, out_qsize 0\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 3 : training on 1548597 raw words (1128495 effective words) took 1.0s, 1088771 effective words/s\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "EPOCH 4 - PROGRESS: at 100.00% examples, 1121957 words/s, in_qsize 0, out_qsize 1\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 4 : training on 1548597 raw words (1129290 effective words) took 1.0s, 1120429 effective words/s\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "EPOCH 5 - PROGRESS: at 100.00% examples, 1119942 words/s, in_qsize 0, out_qsize 1\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 5 : training on 1548597 raw words (1128883 effective words) took 1.0s, 1118749 effective words/s\n",
      "EPOCH 6 - PROGRESS: at 98.20% examples, 1101229 words/s, in_qsize 4, out_qsize 0\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 6 : training on 1548597 raw words (1129119 effective words) took 1.0s, 1101644 effective words/s\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 7 : training on 1548597 raw words (1128746 effective words) took 1.0s, 1130518 effective words/s\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 8 : training on 1548597 raw words (1128823 effective words) took 1.0s, 1130503 effective words/s\n",
      "EPOCH 9 - PROGRESS: at 99.28% examples, 1120008 words/s, in_qsize 2, out_qsize 1\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 9 : training on 1548597 raw words (1129147 effective words) took 1.0s, 1117745 effective words/s\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 10 : training on 1548597 raw words (1128767 effective words) took 1.0s, 1132309 effective words/s\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "EPOCH 11 - PROGRESS: at 100.00% examples, 1123154 words/s, in_qsize 0, out_qsize 1\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 11 : training on 1548597 raw words (1128805 effective words) took 1.0s, 1122034 effective words/s\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "EPOCH 12 - PROGRESS: at 100.00% examples, 1121394 words/s, in_qsize 0, out_qsize 1\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 12 : training on 1548597 raw words (1128577 effective words) took 1.0s, 1120058 effective words/s\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 13 : training on 1548597 raw words (1129111 effective words) took 1.0s, 1131326 effective words/s\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 14 : training on 1548597 raw words (1128614 effective words) took 1.0s, 1129358 effective words/s\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "EPOCH 15 - PROGRESS: at 100.00% examples, 1119200 words/s, in_qsize 0, out_qsize 1\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 15 : training on 1548597 raw words (1129136 effective words) took 1.0s, 1117869 effective words/s\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 16 : training on 1548597 raw words (1128835 effective words) took 1.0s, 1136731 effective words/s\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "EPOCH 17 - PROGRESS: at 100.00% examples, 1127134 words/s, in_qsize 0, out_qsize 1\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 17 : training on 1548597 raw words (1129398 effective words) took 1.0s, 1126050 effective words/s\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "EPOCH 18 - PROGRESS: at 100.00% examples, 1128360 words/s, in_qsize 0, out_qsize 1\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 18 : training on 1548597 raw words (1129067 effective words) took 1.0s, 1127036 effective words/s\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 19 : training on 1548597 raw words (1128300 effective words) took 1.0s, 1130370 effective words/s\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 20 : training on 1548597 raw words (1128613 effective words) took 1.0s, 1130925 effective words/s\n",
      "training on a 30971940 raw words (22578384 effective words) took 20.2s, 1118984 effective words/s\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "# Tokenize Reuters corpus\n",
    "tokenized_docs = [nltk.word_tokenize(reuters.raw(fileid)) for fileid in reuters.fileids()]\n",
    "\n",
    "# Convert tokenized documents to TaggedDocuments\n",
    "tagged_docs = [TaggedDocument(doc, tags=[idx]) for idx, doc in enumerate(tokenized_docs)]\n",
    "\n",
    "# Create and train the doc2vec model. May take a few seconds\n",
    "doc2vec = Doc2Vec(vector_size=300, window=5, min_count=5, dm = 1, epochs=20)\n",
    "\n",
    "# Build the word2vec model from the corpus\n",
    "doc2vec.build_vocab(tagged_docs)\n",
    "\n",
    "doc2vec.train(tagged_docs, epochs=20, total_examples=doc2vec.corpus_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorize the whole doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "Eng_vec = []\n",
    "for i in cleaned:\n",
    "    Eng_vec.append(doc2vec.infer_vector(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# split data into train and test sets\n",
    "seed = 10\n",
    "test_size = 0.4\n",
    "X_train, X_test, y_train, y_test = train_test_split(Eng_vec, label_list, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.17      0.26     14265\n",
      "           1       0.65      0.92      0.76     24326\n",
      "\n",
      "   micro avg       0.64      0.64      0.64     38591\n",
      "   macro avg       0.60      0.54      0.51     38591\n",
      "weighted avg       0.62      0.64      0.58     38591\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6419631520302661"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "LRmodel=LogisticRegression(solver='liblinear')\n",
    "LRmodel = LRmodel.fit(X_train, y_train)\n",
    "\n",
    "y_pred = LRmodel.predict(X_test)\n",
    "\n",
    "\n",
    "target_names = ['0', '1']\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "# check the accuracy on the training set\n",
    "LRmodel.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.17      0.26     14265\n",
      "           1       0.65      0.92      0.76     24326\n",
      "\n",
      "   micro avg       0.64      0.64      0.64     38591\n",
      "   macro avg       0.60      0.54      0.51     38591\n",
      "weighted avg       0.62      0.64      0.58     38591\n",
      "\n",
      "0.6313907387732891\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rdf = RandomForestClassifier(criterion='gini',n_estimators=10, random_state=0, max_depth=10, min_samples_split=10, bootstrap=True)\n",
    "rdf.fit(X_train, y_train)\n",
    "\n",
    "rdf_pred = rdf.predict(X_test)\n",
    "target_names = ['0', '1']\n",
    "\n",
    "print(classification_report(y_test, y_pred, target_names=target_names))\n",
    "\n",
    "rdf_score = rdf.score(X_test, y_test)\n",
    "print(rdf_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example review:\n",
      "   Raw: So ingenious in concept, design and execution that you could watch it on a postage stamp-sized screen and still be engulfed by its charm. \n",
      "\n",
      "   Tokenized: ['Turns', 'out', 'the', 'real', 'magic', 'is', 'nothing', 'to', 'do', 'with', 'technology', ':', 'it', \"'s\", 'in', 'the', 'words', ',', 'the', 'voices', ',', 'the', 'story', '.', 'But', 'then', ',', 'a', 'child', 'could', 'have', 'told', 'you', 'that', '.']\n"
     ]
    }
   ],
   "source": [
    "n = 10 #arbitrary pick\n",
    "print('Example review:\\n   Raw: {} \\n\\n   Tokenized: {}'.format(texts[n], [i for i in tokenized[n]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1.    .  freq:   99871\n",
      " 2.    ,  freq:   94514\n",
      " 3.  the  freq:   94348\n",
      " 4.    a  freq:   65722\n",
      " 5.   of  freq:   57975\n",
      " 6.  and  freq:   57131\n",
      " 7.   to  freq:   36159\n",
      " 8.   's  freq:   34424\n",
      " 9.   is  freq:   33839\n",
      "10.   it  freq:   30807\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "#Note that we convert all tokens to lower case, otherwise words like *The* and *the* are different tokens.\n",
    "token_counter = Counter(token.lower() for sentence in tokenized for token in sentence)\n",
    "top10 = token_counter.most_common()[:10]\n",
    "for i, t in enumerate(top10):\n",
    "    print('{:>2}.{:>5}  freq: {:>7}'.format(i+1, t[0], t[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1.    entire  freq: generation   next:  filmgoers\n",
      " 2.     pixar  freq:    classic   next:        one\n",
      " 3.     woody  freq:      perks   next:    opening\n",
      " 4.introduced  freq:        one   next:        two\n",
      " 5.      easy  freq:        see   next:  virtually\n",
      " 6.    though  freq:  animation   next:      seems\n",
      " 7.   perhaps  freq:       film   next:      meant\n",
      " 8.      time  freq:       kind   next:     future\n",
      " 9.     think  freq:      speak   next:     adults\n",
      "10. ingenious  freq:    concept   next:     design\n"
     ]
    }
   ],
   "source": [
    "top10 = cleaned[:10]\n",
    "for i, t in enumerate(top10):\n",
    "    print('{:>2}.{:>10}  freq: {:>10}   next: {:>10}'.format(i+1, t[0], t[1], t[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "adding document #0 to Dictionary(0 unique tokens: [])\n",
      "adding document #10000 to Dictionary(18915 unique tokens: ['...', 'entire', 'ever', 'filmgoers', 'generation']...)\n",
      "adding document #20000 to Dictionary(27122 unique tokens: ['...', 'entire', 'ever', 'filmgoers', 'generation']...)\n",
      "adding document #30000 to Dictionary(32947 unique tokens: ['...', 'entire', 'ever', 'filmgoers', 'generation']...)\n",
      "adding document #40000 to Dictionary(37856 unique tokens: ['...', 'entire', 'ever', 'filmgoers', 'generation']...)\n",
      "adding document #50000 to Dictionary(42416 unique tokens: ['...', 'entire', 'ever', 'filmgoers', 'generation']...)\n",
      "adding document #60000 to Dictionary(46736 unique tokens: ['...', 'entire', 'ever', 'filmgoers', 'generation']...)\n",
      "adding document #70000 to Dictionary(50443 unique tokens: ['...', 'entire', 'ever', 'filmgoers', 'generation']...)\n",
      "adding document #80000 to Dictionary(53999 unique tokens: ['...', 'entire', 'ever', 'filmgoers', 'generation']...)\n",
      "adding document #90000 to Dictionary(57742 unique tokens: ['...', 'entire', 'ever', 'filmgoers', 'generation']...)\n",
      "built Dictionary(60169 unique tokens: ['...', 'entire', 'ever', 'filmgoers', 'generation']...) from 96476 documents (total 1109695 corpus positions)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "# Create a dictionary from list of documents in order to create BOW model\n",
    "dictionary = corpora.Dictionary(cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting document frequencies\n",
      "PROGRESS: processing document #0\n",
      "PROGRESS: processing document #10000\n",
      "PROGRESS: processing document #20000\n",
      "PROGRESS: processing document #30000\n",
      "PROGRESS: processing document #40000\n",
      "PROGRESS: processing document #50000\n",
      "PROGRESS: processing document #60000\n",
      "PROGRESS: processing document #70000\n",
      "PROGRESS: processing document #80000\n",
      "PROGRESS: processing document #90000\n",
      "calculating IDF weights for 96476 documents and 60169 features (1074117 matrix non-zeros)\n",
      "Example review featurized with Corpus scores : \n",
      "[(\"'re\", 1), (\"'s\", 1), ('alive', 1), ('art', 1), ('comes', 1), ('cowboy', 1), ('form', 1), ('opening', 1), ('perks', 1), ('rebirth', 1), ('scene', 1), ('toy', 1), ('watching', 1), ('woody', 1)]\n",
      "Example review featurized with TF-IDF scores : \n",
      "[(\"'re\", 0.179), (\"'s\", 0.051), ('alive', 0.262), ('art', 0.209), ('comes', 0.197), ('cowboy', 0.316), ('form', 0.231), ('opening', 0.256), ('perks', 0.448), ('rebirth', 0.377), ('scene', 0.214), ('toy', 0.326), ('watching', 0.199), ('woody', 0.247)]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in cleaned]\n",
    "from gensim import corpora, models\n",
    "\n",
    "#Create a TFIDF Model for the corpus\n",
    "tfidf = models.TfidfModel(corpus)\n",
    "print('Example review featurized with Corpus scores : \\n{}'.format([(dictionary[i[0]], i[1]) for i in corpus[2]]))\n",
    "print('Example review featurized with TF-IDF scores : \\n{}'.format([(dictionary[i[0]], round(i[1],3)) for i in tfidf[corpus[2]]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "collecting all words and their counts\n",
      "PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "PROGRESS: at sentence #10000, processed 113993 words, keeping 18915 word types\n",
      "PROGRESS: at sentence #20000, processed 229596 words, keeping 27122 word types\n",
      "PROGRESS: at sentence #30000, processed 339919 words, keeping 32947 word types\n",
      "PROGRESS: at sentence #40000, processed 451212 words, keeping 37856 word types\n",
      "PROGRESS: at sentence #50000, processed 561821 words, keeping 42416 word types\n",
      "PROGRESS: at sentence #60000, processed 676609 words, keeping 46736 word types\n",
      "PROGRESS: at sentence #70000, processed 790298 words, keeping 50443 word types\n",
      "PROGRESS: at sentence #80000, processed 904568 words, keeping 53999 word types\n",
      "PROGRESS: at sentence #90000, processed 1026110 words, keeping 57742 word types\n",
      "collected 60169 word types from a corpus of 1109695 raw words and 96476 sentences\n",
      "Loading a fresh vocabulary\n",
      "effective_min_count=3 retains 25721 unique words (42% of original 60169, drops 34448)\n",
      "effective_min_count=3 leaves 1068008 word corpus (96% of original 1109695, drops 41687)\n",
      "deleting the raw counts dictionary of 60169 items\n",
      "sample=0.001 downsamples 17 most-common words\n",
      "downsampling leaves estimated 994011 word corpus (93.1% of prior 1068008)\n",
      "estimated required memory for 25721 words and 300 dimensions: 74590900 bytes\n",
      "resetting layer weights\n",
      "training model with 3 workers on 25721 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 1 : training on 1109695 raw words (994035 effective words) took 0.8s, 1272487 effective words/s\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 2 : training on 1109695 raw words (994098 effective words) took 0.7s, 1331443 effective words/s\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 3 : training on 1109695 raw words (993932 effective words) took 0.8s, 1308787 effective words/s\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 4 : training on 1109695 raw words (993785 effective words) took 0.8s, 1287851 effective words/s\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 5 : training on 1109695 raw words (994055 effective words) took 0.8s, 1318759 effective words/s\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 6 : training on 1109695 raw words (994006 effective words) took 0.8s, 1322819 effective words/s\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 7 : training on 1109695 raw words (993869 effective words) took 0.8s, 1320732 effective words/s\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 8 : training on 1109695 raw words (993659 effective words) took 0.7s, 1332735 effective words/s\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 9 : training on 1109695 raw words (993842 effective words) took 0.8s, 1311344 effective words/s\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 10 : training on 1109695 raw words (993836 effective words) took 0.8s, 1317163 effective words/s\n",
      "training on a 11096950 raw words (9939117 effective words) took 7.6s, 1303063 effective words/s\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "from gensim import models\n",
    "\n",
    "# Training word2vec model on Gutenberg corpus. This may take a few minutes.\n",
    "model = models.Word2Vec(cleaned,\n",
    "                        size = 300,\n",
    "                        window = 5,\n",
    "                        min_count = 3,\n",
    "                        sg = 0,\n",
    "                        alpha = 0.025,\n",
    "                        iter = 10,\n",
    "                        batch_words = 10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Effective 'alpha' higher than previous training cycles\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training model with 3 workers on 25721 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "worker thread finished; awaiting finish of 2 more threads\n",
      "worker thread finished; awaiting finish of 1 more threads\n",
      "worker thread finished; awaiting finish of 0 more threads\n",
      "EPOCH - 1 : training on 1109695 raw words (993938 effective words) took 0.8s, 1322723 effective words/s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "EPOCH - 1 : supplied example count (96476) did not equal expected count (100000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training on a 1109695 raw words (993938 effective words) took 0.8s, 1313075 effective words/s\n"
     ]
    }
   ],
   "source": [
    "trained_text = model.train(cleaned, total_examples=100000, epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'perks' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-5bfafab7dcd7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m#for i in cleaned:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;31m#for j in i:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcleaned\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m#vector = model.wv['computer']  # numpy vector of a word\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, entities)\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    353\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__contains__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentity\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 471\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    466\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word 'perks' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# split data into train and test sets\n",
    "seed = 0\n",
    "test_size = 0.33\n",
    "word = []\n",
    "#for i in cleaned:\n",
    "    #for j in i:\n",
    "word.append(model.wv[cleaned[2]])\n",
    "\n",
    "#vector = model.wv['computer']  # numpy vector of a word\n",
    "#X_train, X_test, y_train, y_test = train_test_split(model.wv.array, test_size=test_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Example review featurized with TF-IDF scores : \\n{}{}'.format(word[0], number[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from nltk.corpus import reuters\n",
    "\n",
    "from nltk.corpus import gutenberg\n",
    "# Tokenize Reuters corpus\n",
    "new_tokenized_docs = [nltk.word_tokenize(gutenberg.sent) ]\n",
    "\n",
    "# Convert tokenized documents to TaggedDocuments\n",
    "new_tagged_docs = [TaggedDocument(doc, tags=[idx]) for idx, doc in enumerate(tokenized_docs)]\n",
    "\n",
    "# Create and train the doc2vec model. May take a few seconds\n",
    "doc2vec = Doc2Vec(size=300, window=5, min_count=5, dm = 1, iter=10)\n",
    "\n",
    "# Build the word2vec model from the corpus\n",
    "doc2vec.build_vocab(tagged_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "from nltk.corpus import reuters\n",
    "from nltk.corpus import gutenberg\n",
    "from gensim import models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
