{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tianqi Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Load movie reviews dataset\n",
    "df = pd.read_csv( 'data/fake_or_real_news.csv', nrows=100000)\n",
    "title = df.title.values\n",
    "text = df.text.values\n",
    "label = df.label.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['FAKE', 'FAKE', 'REAL', ..., 'FAKE', 'REAL', 'REAL'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find whether a Str is just a number\n",
    "def is_number(s):\n",
    "    try: \n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:  \n",
    "        pass \n",
    "    try:\n",
    "        import unicodedata  \n",
    "        unicodedata.numeric(s)  \n",
    "        return True\n",
    "    except (TypeError, ValueError):\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "\n",
    "#Find whether a Str contain English words or just meanless symbols\n",
    "def containenglish(str0):\n",
    "    import re\n",
    "    return bool(re.search('[a-z]', str0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean non-English news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ChangeLabel(tryText):\n",
    "    if tryText == 'FAKE':\n",
    "        return 0\n",
    "    elif tryText == 'REAL':\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "import types\n",
    "#Separatly get Text with label and Tile with label\n",
    "Text_list = []\n",
    "Text_label = []\n",
    "#clean text\n",
    "for i in range(len(text)): \n",
    "    if containenglish(text[i]) == True:\n",
    "        if (detect(text[i]) == 'en'):\n",
    "            Text_list.append(text[i])\n",
    "            Text_label.append(ChangeLabel(label[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#clean text and title\n",
    "Sum_label = []\n",
    "Title_list = []\n",
    "for i in range(len(text)): \n",
    "    if containenglish(text[i]) == True and containenglish(title[i]) == True:\n",
    "        if (detect(text[i]) == 'en') and (detect(title[i]) == 'en'):\n",
    "            Sum_label.append(ChangeLabel(label[i]))\n",
    "            Title_list.append(title[i]+text[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show TURE news and FAKE news fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "number_real = 0\n",
    "for i in Text_label:\n",
    "    if i == 1:\n",
    "        number_real = number_real + 1\n",
    "AllLabel = len(Text_label)\n",
    "fracREAL = number_real/AllLabel\n",
    "fracFAKE = 1 - fracREAL\n",
    "\n",
    "labels = ['REAL', 'FAKE']\n",
    "values = [fracREAL, fracFAKE]\n",
    "colors = ['red','blue']\n",
    "explode = [0.3,0]\n",
    "\n",
    "plt.title('Data Fraction')\n",
    "plt.pie(values,labels=labels,colors=colors,explode=explode,startangle=90)\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# Transform each review string as a list of token strings. Use text here because text is as same as Eng_list\n",
    "tokenized = [nltk.word_tokenize(review) for review in Text_list]\n",
    "tokenized_title = [nltk.word_tokenize(review) for review in Title_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean stopwords and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from itertools import chain\n",
    "\n",
    "def clean_text(tokenized_list, sw, punct, lemmatize=False):\n",
    "    new_list = []\n",
    "    for doc in tokenized_list:\n",
    "        new_list.append([token.lower() for token in doc if token.lower() not in chain(punct, sw)])\n",
    "    return new_list\n",
    "\n",
    "# Remove punctuations and stopwords, and lower-case text\n",
    "sw = stopwords.words('english')\n",
    "punct = punctuation\n",
    "cleaned = clean_text(tokenized, sw, punct)\n",
    "cleaned_title = clean_text(tokenized_title, sw, punct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean meanless symbols and url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "for idx in range(len(cleaned)):\n",
    "    cleaned[idx] = re.sub(r'@([A-Za-z0-9_]+)', \"\", str(cleaned[idx]))\n",
    "    cleaned[idx] = re.sub(r\"(https|http)\\S+\", \"\", str(cleaned[idx]))\n",
    "    cleaned[idx] = re.sub(r\"”|’|“|–|—\", \"\", str(cleaned[idx]))\n",
    "    \n",
    "for idx in range(len(cleaned_title)):\n",
    "    cleaned_title[idx] = re.sub(r'@([A-Za-z0-9_]+)', \"\", str(cleaned_title[idx]))\n",
    "    cleaned_title[idx] = re.sub(r\"(https|http)\\S+\", \"\", str(cleaned_title[idx]))\n",
    "    cleaned_title[idx] = re.sub(r\"”|’|“|–|—\", \"\", str(cleaned_title[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# Update puncuation list in spacy\n",
    "nlp.vocab[\"$\"].is_punct = True\n",
    "nlp.vocab[\"|\"].is_punct = True\n",
    "nlp.vocab[\"+\"].is_punct = True\n",
    "nlp.vocab[\"<\"].is_punct = True\n",
    "nlp.vocab[\">\"].is_punct = True\n",
    "nlp.vocab[\"=\"].is_punct = True\n",
    "nlp.vocab[\"^\"].is_punct = True\n",
    "nlp.vocab[\"`\"].is_punct = True\n",
    "nlp.vocab[\"~\"].is_stop = True\n",
    "nlp.vocab[\"s\"].is_stop = True\n",
    "#nlp.vocab[\"t\"].is_stop = True # change to not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatize text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to eliminate tokens that are pure punctuation, whitespace, or stopword\n",
    "# can be updated based on desired filtering \n",
    "\n",
    "def process_txt(token):\n",
    "    return token.is_punct or token.is_space or token.is_stop or token.like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to take array of articles and turn them into nested list of tokens\n",
    "def lemmatize_txt(array):\n",
    "    lemma = []\n",
    "    \n",
    "    for doc in nlp.pipe(array, batch_size=50,\n",
    "                        n_threads=-1):\n",
    "        if doc.is_parsed:\n",
    "            lemma.append([n.lemma_ for n in doc if not process_txt(n)])\n",
    "        \n",
    "        else:\n",
    "            lemma.append(None)\n",
    "    \n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create nested list of tokens for each article\n",
    "lem = lemmatize_txt(cleaned)\n",
    "lem_title = lemmatize_txt(cleaned_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert list of list to list of string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "newLem = []\n",
    "newLem_title = []\n",
    "for doc in lem: \n",
    "    finalString = ', '.join(doc)\n",
    "    newLem.append(finalString)\n",
    "    \n",
    "for doc in lem_title: \n",
    "    finalString = ', '.join(doc)\n",
    "    newLem_title.append(finalString)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jiahao Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing the text， using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#validation set\n",
    "\n",
    "#split data into training set and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X1 = newLem\n",
    "Y1 = Text_label\n",
    "X2 = newLem_title\n",
    "Y2 = Sum_label\n",
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(X1, Y1, test_size = 0.33, stratify = Y1)\n",
    "\n",
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(X2, Y2, test_size = 0.33, stratify = Y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tdf1 = TfidfVectorizer()\n",
    "tdf2 = TfidfVectorizer()\n",
    "x_tfidf_train1 = tdf1.fit_transform(x_train1)\n",
    "x_tfidf_test1 = tdf1.transform(x_test1)\n",
    "x_tfidf_train2 = tdf2.fit_transform(x_train2)\n",
    "x_tfidf_test2 = tdf2.transform(x_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8942376096751244\n",
      "0.8431183830606352\n"
     ]
    }
   ],
   "source": [
    "#assess this model,using naive bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "tdf_model = MultinomialNB().fit(x_tfidf_train1, y_train1)\n",
    "train_score = tdf_model.score(x_tfidf_train1, y_train1)\n",
    "test_score = tdf_model.score(x_tfidf_test1, y_test1)\n",
    "print(train_score)\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing the text， using doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the dictionary and corpus\n",
    "from gensim import corpora\n",
    "\n",
    "dictionary1 = corpora.Dictionary([cleaned])\n",
    "dictionary2 = corpora.Dictionary([cleaned_title])\n",
    "corpus1 = [dictionary1.doc2bow([text]) for text in cleaned]\n",
    "corpus2 = [dictionary2.doc2bow([text]) for text in cleaned_title]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a vocabulary\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "vocabulary1 = [TaggedDocument(doc, tags=[idx]) for idx, doc in enumerate(cleaned)]\n",
    "vocabulary2 = [TaggedDocument(doc, tags=[idx]) for idx, doc in enumerate(cleaned_title)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "import numpy as np\n",
    "doc2vec1 = Doc2Vec(vector_size=300, window=5, min_count=5, dm = 1, epochs=10)\n",
    "doc2vec2 = Doc2Vec(vector_size=300, window=5, min_count=5, dm = 1, epochs=10)\n",
    "doc2vec1.build_vocab(vocabulary1)\n",
    "doc2vec2.build_vocab(vocabulary2)\n",
    "\n",
    "#Train the d2v model\n",
    "doc2vec1.train(vocabulary1, epochs=10, total_examples=doc2vec1.corpus_count)\n",
    "doc2vec2.train(vocabulary2, epochs=10, total_examples=doc2vec2.corpus_count)\n",
    "\n",
    "#Build a new matrix that can fit classifier\n",
    "doc2vec_list1 = np.zeros((len(cleaned),300))\n",
    "doc2vec_list2 = np.zeros((len(cleaned_title),300))\n",
    "for i in range (len(cleaned)):\n",
    "    doc2vec_list1[i] = doc2vec1.infer_vector([cleaned[i]])\n",
    "    \n",
    "for i in range (len(cleaned_title)):\n",
    "    doc2vec_list2[i] = doc2vec2.infer_vector([cleaned_title[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1 = doc2vec_list1\n",
    "Y1 = Text_label\n",
    "X2 = doc2vec_list2\n",
    "Y2 = Sum_label\n",
    "x_train1, x_test1, y_train1, y_test1 = train_test_split(X1, Y1, test_size = 0.33, stratify = Y1)\n",
    "x_train2, x_test2, y_train2, y_test2 = train_test_split(X2, Y2, test_size = 0.33, stratify = Y2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hongyu Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Xgboost Classfier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting package metadata (repodata.json): ...working... done\n",
      "Solving environment: ...working... failed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - xgboost\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - https://conda.anaconda.org/conda-forge/win-64\n",
      "  - https://conda.anaconda.org/conda-forge/noarch\n",
      "  - https://repo.anaconda.com/pkgs/main/win-64\n",
      "  - https://repo.anaconda.com/pkgs/main/noarch\n",
      "  - https://repo.anaconda.com/pkgs/r/win-64\n",
      "  - https://repo.anaconda.com/pkgs/r/noarch\n",
      "  - https://repo.anaconda.com/pkgs/msys2/win-64\n",
      "  - https://repo.anaconda.com/pkgs/msys2/noarch\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install -c conda-forge xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pipi\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.5055387713997986"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model = LogisticRegression()\n",
    "model = model.fit(x_train2, y_train2)\n",
    "\n",
    "y_pred = model.predict(x_test2)\n",
    "\n",
    "# check the accuracy on the training set\n",
    "model.score(x_test2, y_test2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "Not Admitted       0.49      0.46      0.47       982\n",
      "    Admitted       0.50      0.53      0.52      1004\n",
      "\n",
      "   micro avg       0.49      0.49      0.49      1986\n",
      "   macro avg       0.49      0.49      0.49      1986\n",
      "weighted avg       0.49      0.49      0.49      1986\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "XGmodel = XGBClassifier(max_depth=10, learning_rate=0.3, \n",
    "                        n_estimators=1000, silent=True, \n",
    "                        objective='binary:logistic', nthread=-1, \n",
    "                        gamma=0, min_child_weight=1, max_delta_step=0, \n",
    "                        subsample=0.7, colsample_bytree=1, \n",
    "                        colsample_bylevel=1, reg_alpha=0,            \n",
    "                        reg_lambda=1, scale_pos_weight=1, \n",
    "                        base_score=0.5, seed=0, missing=None)\n",
    "\n",
    "XGmodel.fit(x_train2, y_train2)\n",
    "\n",
    "y_pred = XGmodel.predict(x_test2)\n",
    "\n",
    "target_names = ['Not Admitted', 'Admitted']\n",
    "\n",
    "print(classification_report(y_test2, y_pred, target_names=target_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
