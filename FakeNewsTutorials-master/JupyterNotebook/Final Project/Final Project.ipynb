{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#Load movie reviews dataset\n",
    "df = pd.read_csv( 'fake_or_real_news.csv', nrows=100000)\n",
    "title = df.title.values\n",
    "text = df.text.values\n",
    "label = df.label.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['FAKE', 'FAKE', 'REAL', ..., 'FAKE', 'REAL', 'REAL'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find whether a Str is just a number\n",
    "def is_number(s):\n",
    "    try: \n",
    "        float(s)\n",
    "        return True\n",
    "    except ValueError:  \n",
    "        pass \n",
    "    try:\n",
    "        import unicodedata  \n",
    "        unicodedata.numeric(s)  \n",
    "        return True\n",
    "    except (TypeError, ValueError):\n",
    "        pass\n",
    "    return False\n",
    "\n",
    "\n",
    "#Find whether a Str contain English words or just meanless symbols\n",
    "def containenglish(str0):\n",
    "    import re\n",
    "    return bool(re.search('[a-z]', str0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean non-English news, useless here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langdetect import detect\n",
    "import types\n",
    "#Separatly get Text with label and Tile with label\n",
    "Eng_list = []\n",
    "Text_label = []\n",
    "\n",
    "Title_list = []\n",
    "Title_label = []\n",
    "#clean text\n",
    "for i in range(len(text)): \n",
    "    if containenglish(text[i]) == True and containenglish(title[i]) == True:\n",
    "        if (detect(text[i]) == 'en') and (detect(title[i]) == 'en'):\n",
    "            Eng_list.append(text[i])\n",
    "            Text_label.append(label[i])\n",
    "            Title_list.append(title[i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# Transform each review string as a list of token strings. Use text here because text is as same as Eng_list\n",
    "tokenized = [nltk.word_tokenize(review) for review in Eng_list]\n",
    "tokenized_title = [nltk.word_tokenize(review) for review in Title_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from itertools import chain\n",
    "\n",
    "def clean_text(tokenized_list, sw, punct, lemmatize=False):\n",
    "    new_list = []\n",
    "    for doc in tokenized_list:\n",
    "        new_list.append([token.lower() for token in doc if token.lower() not in chain(punct, sw)])\n",
    "    return new_list\n",
    "\n",
    "# Remove punctuations and stopwords, and lower-case text\n",
    "sw = stopwords.words('english')\n",
    "punct = punctuation\n",
    "cleaned = clean_text(tokenized, sw, punct)\n",
    "cleaned_title = clean_text(tokenized_title, sw, punct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "for idx in range(len(cleaned)):\n",
    "    cleaned[idx] = re.sub(r'@([A-Za-z0-9_]+)', \"\", str(cleaned[idx]))\n",
    "    cleaned[idx] = re.sub(r\"(https|http)\\S+\", \"\", str(cleaned[idx]))\n",
    "    cleaned[idx] = re.sub(r\"”|’|“|–|—\", \"\", str(cleaned[idx]))\n",
    "    \n",
    "for idx in range(len(cleaned_title)):\n",
    "    cleaned_title[idx] = re.sub(r'@([A-Za-z0-9_]+)', \"\", str(cleaned_title[idx]))\n",
    "    cleaned_title[idx] = re.sub(r\"(https|http)\\S+\", \"\", str(cleaned_title[idx]))\n",
    "    cleaned_title[idx] = re.sub(r\"”|’|“|–|—\", \"\", str(cleaned_title[idx]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "num_label = np.where(Text_label == 'FAKE', 0 , 1)\n",
    "num_label = np.where(Title_label == 'FAKE', 0 , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "# Update puncuation list in spacy\n",
    "nlp.vocab[\"$\"].is_punct = True\n",
    "nlp.vocab[\"|\"].is_punct = True\n",
    "nlp.vocab[\"+\"].is_punct = True\n",
    "nlp.vocab[\"<\"].is_punct = True\n",
    "nlp.vocab[\">\"].is_punct = True\n",
    "nlp.vocab[\"=\"].is_punct = True\n",
    "nlp.vocab[\"^\"].is_punct = True\n",
    "nlp.vocab[\"`\"].is_punct = True\n",
    "nlp.vocab[\"~\"].is_stop = True\n",
    "nlp.vocab[\"s\"].is_stop = True\n",
    "nlp.vocab[\"t\"].is_stop = True # change to not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper function to eliminate tokens that are pure punctuation, whitespace, or stopword\n",
    "# can be updated based on desired filtering \n",
    "\n",
    "def process_txt(token):\n",
    "    return token.is_punct or token.is_space or token.is_stop or token.like_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to take array of articles and turn them into nested list of tokens\n",
    "def lemmatize_txt(array):\n",
    "    lemma = []\n",
    "    \n",
    "    for doc in nlp.pipe(array, batch_size=50,\n",
    "                        n_threads=-1):\n",
    "        if doc.is_parsed:\n",
    "            lemma.append([n.lemma_ for n in doc if not process_txt(n)])\n",
    "        \n",
    "        else:\n",
    "            lemma.append(None)\n",
    "    \n",
    "    return lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create nested list of tokens for each article\n",
    "lem = lemmatize_txt(cleaned)\n",
    "lem_title = lemmatize_txt(cleaned_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "newLem = []\n",
    "newLem_title = []\n",
    "for doc in lem: \n",
    "    finalString = ', '.join(doc)\n",
    "    newLem.append(finalString)\n",
    "    \n",
    "for doc in lem_title: \n",
    "    finalString = ', '.join(doc)\n",
    "    newLem_title.append(finalString)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'watch, exact, moment, paul, ryan, commit, political, suicide, trump, rally, video'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newLem_title[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing the text， using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6008"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(newLem.size)\n",
    "type(label)\n",
    "len(newLem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data into training set and test set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = newLem\n",
    "Y = Text_label\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.33, stratify = Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tdf = TfidfVectorizer()\n",
    "x_tfidf_train = tdf.fit_transform(x_train)\n",
    "x_tfidf_test = tdf.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8877018633540372\n",
      "0.8204740292486132\n"
     ]
    }
   ],
   "source": [
    "#assess this model,using naive bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "tdf_model = MultinomialNB().fit(x_tfidf_train, y_train)\n",
    "train_score = tdf_model.score(x_tfidf_train, y_train)\n",
    "test_score = tdf_model.score(x_tfidf_test, y_test)\n",
    "print(train_score)\n",
    "print(test_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocessing the text， using doc2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build the dictionary and corpus\n",
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary([cleaned])\n",
    "corpus = [dictionary.doc2bow([text]) for text in cleaned]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build a vocabulary\n",
    "from gensim.models.doc2vec import TaggedDocument\n",
    "vocabulary = [TaggedDocument(doc, tags=[idx]) for idx, doc in enumerate(cleaned)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Doc2Vec\n",
    "doc2vec = Doc2Vec(vector_size=300, window=5, min_count=5, dm = 1, epochs=10)\n",
    "doc2vec.build_vocab(vocabulary)\n",
    "\n",
    "#Train the d2v model\n",
    "doc2vec.train(vocabulary, epochs=10, total_examples=doc2vec.corpus_count)\n",
    "\n",
    "#Build a new matrix that can fit classifier\n",
    "doc2vec_list = np.zeros((len(cleaned),300))\n",
    "for i in range (len(cleaned)):\n",
    "    doc2vec_list[i] = doc2vec.infer_vector([cleaned[i]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = doc2vec_list\n",
    "Y = label\n",
    "x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size = 0.33, stratify = Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
